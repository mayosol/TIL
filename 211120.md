2021.11.20
=====
딥러닝 1단계: 신경망과 딥러닝
-----
- 머신러닝은 feature extraction을 사람이 직접 해주어야 하는데 딥러닝은 기계가 스스로 학습한다.
- 데이터의 양과 신경망의 크기가 커짐에 따라 딥러닝의 성능도 좋아졌다. 규모가 딥러닝의 발전을 이끈 셈이다.
- 데이터의 양이 적다면 알고리즘의 상대적 순위가 정해져있지 않다. 구현 방법(피처와 알고리즘을 다루는 방식)에 따라 성능이 결정된다.
- 로지스틱 회귀에서 손실함수로 MSE를 사용하면 최적화 함수가 볼록하지 않으므로 여러 개의 지역 최적값을 가질 수 있어 문제가 생긴다. -> 이해를 잘 못했다. 이유를 찾아봐야겠다.
- 로지스틱 회귀에서 사용하는 손실함수는 아래와 같다.
$$
 L(\hat{y},y)=−(ylog\hat{y}+(1−y)log(1−\hat{y})) 
$$
- 벡터화로 for loop 없애기 가능 - 몇 백배 빨라진다.
  
***

알고리즘 공부
----
### 해시 - 베스트 앨범, level3
와 정말 오래 걸렸다. 원래는 월요일까지 풀었어야 했지만 시간이 부족해서 못 풀었던 문제를 싹 지우고 다시 풀어봤다. 처음으로 defaultdict도 사용해보았고 dictionary 타입에 대해서 많이 공부할 수 있었다. defaultdict를 사용하니 key값이 있는지 없는지 확인하지 않아도 값을 변경할 수 있어서 편했다. 그런데 dictionary는 dict의 원래 형태를 유지한 채로 key나 value 기준으로 정렬할 수 있는 방법이 없는걸까? 이 부분에 대해 알아보다가 시간이 너무 많이 지나버렸고 아직도 찾지 못했다... key나 value로 정렬하면 그 값만 리스트로 반환되고, items로 정렬하면 tuple로 나오는 것이 참 불편하다. 그래도 불굴의 의지로다가 2시간 만에! 풀게되어서 너무 기쁘다. 처음에는 level3 치고는 조금 쉽다고 생각했는데 :sweat: 역시 다 이유가 있었다. 오늘은 여기까지만 풀고 다른 문제는 내일 더 풀어봐야겠다.

***

오늘의 일기
-----
딥러닝을 처음 공부하기 시작했을 때 앤드류응 교수님의 똑같은 강의를 들었었다. 그때는 필기 집착증이 심해서 초 단위로 세세하게 정리했는데 이번에는 1.5배속으로 끊지 않고 쭉 들어보았다. 이게 무슨 말일까 했던 부분도 이제는 나름 이해가 가는 게 신기하기도 하고 뿌듯했다:thumbsup: 그동안 열심히 공부해서 조금 성장한 것도 맞겠지만 그때는 오히려 너무 촘촘히 정리하다 보니 전체적인 흐름을 보지 못한 것 같기도 하다. 원래는 오늘까지 chapter1을 다 들었어야 했는데 아직 조금 남았다. 내일 후딱 듣자! 아 그리고 오늘은 카페에서 공부하고 샤브샤브 먹었지 :heart: 월남쌈 너 정말 맛있어...

### :star: 내일 할 일 :star:
- 팀 레포트 완성 및 제출
- 알고리즘 스택/큐 : 기능개발, 프린터, 다리를 지나는 트럭, 주식가격 풀기
- 앤드류 응 딥러닝 1단계: 신경망과 딥러닝 완강
- pm 11시 음성팀 회의